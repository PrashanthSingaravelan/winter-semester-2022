summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
classifier_knn <- knn(train=df_train, test=df_test, cl=df_train$Species, k=round(sqrt(nrow(df_train))))
classifier_knn <- knn(train=df_train, test=df_test, cl=df_train$Species, k=3)
summary(classifier_knn)
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
classifier_knn <- knn(train=df_train, test=df_test, cl=df_train$Species, k=3)
df_train$Species
library(dplyr)
x
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
classifier_knn <- knn(train=df_train, test=df_test, cl=df_train$Species, k=3)
x = df %>% select(~Species)
x = df %>% select(., ~Species)
x = df %>% select(where(~Species))
x = df %>% select(where(~Species))
x = df %>% select(-Species)
x
## dependent feature
y = df %>% select(Species)
library(cluster)
## dependent feature
y = df_train %>% select(Species)
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
k_means$size
k_means$cluster
## Confusion Matrix
cm <- table(df_test$Species, kmeans$cluster)
## Confusion Matrix
cm <- table(df_test$Species, k_means$cluster)
## dependent feature
y = df %>% select(Species)
k_means
k_means
```{r}
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(df_test$Species, k_means$cluster)
## Confusion Matrix
cm <- table(df$Species, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=df$Species)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=df$Species)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
classifier_knn <- knn(train=df_train, test=df_test, cl=df_train$Species, k=3)
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df %>% select(Species)
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df %>% select(Species)
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
## dependent feature
y = df %>% select(Species)
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df %>% select(Species)
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
## Confusion Matrix
cm <- table(x, k_means$cluster)
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df %>% select(Species)
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(x, k_means$cluster)
## Confusion Matrix
cm <- table(y, k_means$cluster)
## Confusion Matrix
cm <- table(df$Species, k_means$cluster)
## dependent feature
y = df$Species
## Confusion Matrix
cm <- table(y, k_means$cluster)
```{r}
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
df %>% select(-Species)
df$Species
df %>% select(Species)
type(df$Species)
dim(df$Species)
class(df$Species)
class(df %>% select(Species))
myclust=hclust(iris_dist,method="complete")
myclust=hclust(x,method="complete")
## Finding distance matrix
x_std_dist = dist(x_std_data)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust
myclust = hclust(x_std_dist,method="complete")
myclust
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(Hierar_cl, k = 3 )
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
library(RColorBrewer)
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
head(df)
df = data("Groceries")
head(df)
df = data("Groceries")
df
Groceries
view(df)
df
# using apriori() function
rules <- apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
df = data("Groceries")
df
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
df = "Groceries"
head(df)
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
df = data('Groceries')
head(df)
# using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
# using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
## using inspect() function
inspect(rules[1:10])
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
## using inspect() function
inspect(rules[1:10])
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
df_train[, 1:4]
classifier_knn <- knn(train = df_train_scale, test = df_test_scale, cl = df_train$Species,k = 3)
df_train_scale = scale(df_train[, 1:4])
df_test_scale = scale(df_test[, 1:4])
classifier_knn <- knn(train = df_train_scale, test = df_test_scale, cl = df_train$Species,k = 3)
classifier_knn
cm <- table(df_test$Species, classifier_knn)
cm
misClassError <- mean(classifier_knn != df_test$Species)
print(paste('Accuracy =', 1-misClassError))
print(paste('Accuracy =', misClassError))
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
