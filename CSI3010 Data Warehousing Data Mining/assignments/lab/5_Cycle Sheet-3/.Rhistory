cm
misClassError <- mean(classifier_knn != df_test$Species)
print(paste('Accuracy =', 1-misClassError))
print(paste('Accuracy =', misClassError))
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
pred
table(pred)
table(df_test$Species)
table(pred,df_test$Species)
x <- df$Sepal.Width
y <- df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## Scaling the independent feature values for both training and testing data
df_train_scale = scale(df_train[, 1:4])
df_test_scale  = scale(df_test[, 1:4])
classifier_knn <- knn(train = df_train_scale, test = df_test_scale,
cl = df_train$Species,k = 3)
classifier_knn
## confusion matrix
cm <- table(df_test$Species, classifier_knn)
cm
## accuracy of the classifier
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
pred
table(pred)
cat("Predicted value")
table(pred)
cat("Actual value")
table(df_test$Species)
cat("Predicted value from the trainning data")
table(pred)
cat("Actual value in the test-data")
table(df_test$Species)
## Confusion matrix
table(pred,df_test$Species)
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
cat("Predicted value from the trainning data")
table(pred)
cat("Actual value in the test-data")
table(df_test$Species)
cat("Predicted value from the trainning data")
table(pred)
cat("\nActual value in the test-data")
table(df_test$Species)
x = df$Sepal.Width
y = df$Sepal.Length
linear_model = lm(x~y, data=df)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## Dependent and In-Dependent event
x = df_train$Sepal.Width
y = df_train$Sepal.Length
linear_model = lm(x~y, data=df_train)
summary(linear_model)
## Separating into Dependent and In-Dependent attributes
x = df_train$Sepal.Width
y = df_train$Sepal.Length
linear_model = lm(x~y, data=df_train)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",
abline(linear_model),cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
cat("Predicted value from the trainning data")
table(pred)
cat("\nActual value in the test-data")
table(df_test$Species)
## Confusion matrix
table(pred,df_test$Species)
## Separating into Dependent and In-Dependent attributes
x = df_train$Sepal.Width
y = df_train$Sepal.Length
## Fitting the trainning data into the model
linear_model = lm(x~y, data=df_train)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",abline(linear_model),
cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## Scaling the independent feature values for both training and testing data
df_train_scale = scale(df_train[, 1:4])
df_test_scale  = scale(df_test[, 1:4])
## Fitting the trainning data into the model
classifier_knn <- knn(train = df_train_scale, test = df_test_scale,
cl = df_train$Species,k = 3)
classifier_knn
## confusion matrix
cm <- table(df_test$Species, classifier_knn)
cm
## accuracy of the classifier
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
data('Groceries')
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
## using inspect() function
inspect(rules[1:10])
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
cat("Predicted value from the trainning data")
table(pred)
cat("\nActual value in the test-data")
table(df_test$Species)
## Confusion matrix
table(pred,df_test$Species)
## Separating into Dependent and In-Dependent attributes
x = df_train$Sepal.Width
y = df_train$Sepal.Length
## Fitting the trainning data into the model
linear_model = lm(x~y, data=df_train)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",abline(linear_model),
cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## Scaling the independent feature values for both training and testing data
df_train_scale = scale(df_train[, 1:4])
df_test_scale  = scale(df_test[, 1:4])
## Fitting the trainning data into the model
classifier_knn <- knn(train = df_train_scale, test = df_test_scale,
cl = df_train$Species,k = 3)
classifier_knn
## confusion matrix
cm <- table(df_test$Species, classifier_knn)
cm
## accuracy of the classifier
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
data('Groceries')
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
## using inspect() function
inspect(rules[1:10])
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = iris
head(df)
str(df)
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))
df_train = df[split, ]
df_test  = df[!split, ]
print(nrow(df_train))
print(nrow(df_test))
## Fitting the training data into the model
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
cat("Predicted value from the trainning data")
table(pred)
cat("\nActual value in the test-data")
table(df_test$Species)
## Confusion matrix
table(pred,df_test$Species)
## Separating into Dependent and In-Dependent attributes
x = df_train$Sepal.Width
y = df_train$Sepal.Length
## Fitting the trainning data into the model
linear_model = lm(x~y, data=df_train)
summary(linear_model)
result = predict(linear_model,df_test)
summary(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",abline(linear_model),
cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
## Scaling the independent feature values for both training and testing data
df_train_scale = scale(df_train[, 1:4])
df_test_scale  = scale(df_test[, 1:4])
## Fitting the trainning data into the model
classifier_knn <- knn(train = df_train_scale, test = df_test_scale,
cl = df_train$Species,k = 3)
classifier_knn
## confusion matrix
cm <- table(df_test$Species, classifier_knn)
cm
## accuracy of the classifier
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
## in-dependent feature
x = df %>% select(-Species)
## dependent feature
y = df$Species
k_means = kmeans(x, centers = 3, nstart = 20)
k_means
## Size of each cluster
k_means$size
## Cluster identification for each observation
k_means$cluster
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
plot(iris[c("Petal.Length","Petal.Width")],col=y)
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
## Standardizing the values
x_std_data = scale(x)
## Finding distance matrix
x_std_dist = dist(x_std_data)
myclust = hclust(x_std_dist,method="complete")
myclust
## Plotting dendogram
## Before cutting
plot(myclust)
## Plotting dendogram
## After cutting
plot(myclust)
## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
table(fit)
rect.hclust(myclust, k = 3, border = "green")
data('Groceries')
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
## using inspect() function
inspect(rules[1:10])
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
df = read.csv('data.csv')
head(df)
nrow(df)
ncol(df)
## Data-Type of all the attributes
str(df)
## using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
rules
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(df, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
library(datasets)
library(ggplot2)
library(caTools)
library(class)
library(dplyr)
library(ClusterR)
library(cluster)
library(arules)
library(arulesViz)
library(RColorBrewer)
df = read.csv('data.csv')
head(df)
nrow(df)
ncol(df)
## Data-Type of all the attributes
str(df)
## using apriori() function
rules = apriori(df, parameter = list(supp = 0.01, conf = 0.2))
rules
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(df, topN = 20,
col = brewer.pal(8, 'Pastel2'),
main = 'Relative Item Frequency Plot',
type = "relative",
ylab = "Item Frequency (Relative)")
rules2matrix()
rules
