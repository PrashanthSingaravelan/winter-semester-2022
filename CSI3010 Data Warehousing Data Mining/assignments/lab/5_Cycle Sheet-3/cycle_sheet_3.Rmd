---
title: "19MID0020 Cycle-Sheet:3"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

## Importing the libraries
```{r}
library(datasets)
library(ggplot2)
library("e1071") ## Naive-Bayes classifier

library(caTools)
library(class)

library(dplyr)
library(ClusterR)
library(cluster)

library(arules)
library(arulesViz)
library(RColorBrewer)
```

## Importing the data-set
```{r}
df = iris
head(df)
```

## Pre-Processing the data-set
```{r}
str(df)
```

```{r}
## Number of NaN values in the data-set
sum(is.nan(as.matrix(df)))
```
## Splitting into train and test data
```{r}
n = nrow(df)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.75, 0.25))

df_train = df[split, ]
df_test  = df[!split, ]
```

```{r}
print(nrow(df_train))
print(nrow(df_test))
```
## 1)Naive Bayes Classifier
```{r}
## Fitting the training data into the model
model = naiveBayes(Species~.,data=df_train)
pred  = predict(model,df_test)
```

```{r}
cat("Predicted value from the trainning data")
table(pred)

cat("\nActual value in the test-data")
table(df_test$Species)
```

```{r}
## Confusion matrix
table(pred,df_test$Species)
```

## 2)Linear Regression
```{r}
## Separating into Dependent and In-Dependent attributes
x = df_train$Sepal.Width 
y = df_train$Sepal.Length

## Fitting the trainning data into the model
linear_model = lm(x~y, data=df_train)
summary(linear_model)
```

```{r}
result = predict(linear_model,df_test)
summary(result)
```
```{r}
# Plot the chart.
plot(y,x,col = "blue",main = "Sepal Width and Sepal Length Regression",abline(linear_model),
     cex = 1.3,pch = 16, xlab = "Sepal Width", ylab = "Sepal Length")
```

## 3)K-Nearest Neighbor Classifier
```{r}
## Scaling the independent feature values for both training and testing data
df_train_scale = scale(df_train[, 1:4])
df_test_scale  = scale(df_test[, 1:4])
```

```{r}
## Fitting the trainning data into the model
classifier_knn <- knn(train = df_train_scale, test = df_test_scale, 
                      cl = df_train$Species,k = 3)
classifier_knn
```

```{r}
## confusion matrix
cm <- table(df_test$Species, classifier_knn)
cm
```

```{r}
## accuracy of the classifier
misClassError <- mean(classifier_knn == df_test$Species)
print(paste('Accuracy =', misClassError))
```

## 4)K-Means Clustering
```{r}
## in-dependent feature
x = df %>% select(-Species)

## dependent feature
y = df$Species
```

```{r}
k_means = kmeans(x, centers = 3, nstart = 20)

k_means
```

```{r}
## Size of each cluster
k_means$size
```

```{r}
## Cluster identification for each observation
k_means$cluster
```

```{r}
## Confusion Matrix
cm <- table(y, k_means$cluster)
cm
```

```{r}
plot(iris[c("Petal.Length","Petal.Width")],col=k_means$cluster)
```

```{r}
plot(iris[c("Petal.Length","Petal.Width")],col=y)
```

```{r}
plot(iris[c("Sepal.Length","Sepal.Width")],col=k_means$cluster)
```

## 5)Hierarchial Clustering
```{r}
## Standardizing the values
x_std_data = scale(x)

## Finding distance matrix
x_std_dist = dist(x_std_data)
```

```{r}
myclust = hclust(x_std_dist,method="complete")
myclust
```

```{r} 
## Plotting dendogram
## Before cutting
plot(myclust)
```

```{r}
## Plotting dendogram
## After cutting
plot(myclust)

## Choosing no. of clusters
# Cutting tree by height
abline(h = 110, col = "green")

# Cutting tree by no. of clusters
fit <- cutree(myclust, k = 3 )
fit
 
table(fit)
rect.hclust(myclust, k = 3, border = "green")
```

## 6)Association Rule mining using Apriori Algorithms

```{r}
data('Groceries')
```

```{r}
## using apriori() function
rules = apriori(Groceries, parameter = list(supp = 0.01, conf = 0.2))
rules
```

```{r}
## using inspect() function
inspect(rules[1:10])
```


```{r}
# using itemFrequencyPlot() function
arules::itemFrequencyPlot(Groceries, topN = 20,
                          col = brewer.pal(8, 'Pastel2'),
                          main = 'Relative Item Frequency Plot',
                          type = "relative",
                          ylab = "Item Frequency (Relative)")
```